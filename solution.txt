Lab | Data Transformations

In today's lesson, we talked about continuous distributions (mainly normal distribution), linear regression, and how multicollinearity can impact the model. In this lab, we will test your knowledge of those things using the marketing_customer_analysis.csv file. You can continue using the same Jupyter file. The file can be found in the files_for_lab folder.

Get the data
Use the Jupyter file from the last lab (Customer Analysis Round 3)

Complete the following task
Check the data types of the columns. Get the numeric data into dataframe called numerical and categorical columns in a dataframe called categoricals. (You can use np.number and np.object to select the numerical data types and categorical data types respectively)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# The line down below is needed to prevent matplotlib to open the graph in a seaparate popup window. 
%matplotlib inline

data = pd.read_csv('marketing_customer_analysis.csv')
data

data.dtypes

numeric_columns = data.select_dtypes(include=['int','float']).columns
numerical = data[numeric_columns]
numerical

categorical_columns = data.select_dtypes(include=['object','category']).columns
categorical = data[categorical_columns]
categorical

Now we will try to check the normality of the numerical variables visually
Use seaborn library to construct distribution plots for the numerical variables
Use Matplotlib to construct histograms
Do the distributions for different numerical columns look symmetrical? Compute the skewness for each, and add a comment with your findings.

sns.displot(numerical['Customer Lifetime Value'])
plt.show()

numerical['Customer Lifetime Value'].hist(bins=50)
plt.show()

sns.pairplot(numerical)
plt.show()

sns.displot(numerical['Income'])
plt.show()
display(data['Income'].skew())
sns.displot(numerical['Monthly Premium Auto'])
plt.show()
display(data['Monthly Premium Auto'].skew())

#The distributions for these two numerical columns look symmetrical.

For the numerical variables, check the multicollinearity between the features. Please note that we will use the column total_claim_amount later as the target variable.

# load statmodels functions
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# compute the vif for all given features
def compute_vif(considered_features):
    
    X = numerical[considered_features]
    # the calculation of variance inflation requires a constant
    X['intercept'] = 1
    
    # create dataframe to store vif values
    vif = pd.DataFrame()
    vif["Variable"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    vif = vif[vif['Variable']!='intercept']
    return vif

 # features to consider removing
considered_features = ['Income', 'Monthly Premium Auto']


# compute vif 
compute_vif(considered_features).sort_values('VIF', ascending=False)

If you find a pair of columns that show a high correlation between them (greater than 0.9), drop the one that is less correlated with the column total_claim_amount. Write code for both the correlation matrix. If there is no pair of features that have a high correlation, then do not drop any features.

# features to consider removing
considered_features = ['Income', 'Total Claim Amount']


# compute vif 
compute_vif(considered_features).sort_values('VIF', ascending=False)

df = pd.DataFrame(numerical)
corr_matrix = df.corr()
corr_matrix

Plot the heatmap of the correlation matrix after the filtering.

plt.figure(figsize=(10, 8))  # Set the figure size
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")

# Add title and display the plot
plt.title('Correlation Matrix Heatmap')
plt.show()




